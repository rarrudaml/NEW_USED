{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8bbe1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import zipfile\n",
    "\n",
    "from datetime import datetime\n",
    "from pandas import json_normalize\n",
    "\n",
    "####### Predict Title\n",
    "\n",
    "from replib.descriptors.metaprod2vec import MetaProd2Vec\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575880e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "# You can safely assume that `build_dataset` is correctly implemented\n",
    "def build_dataset():\n",
    "    data = [json.loads(x) for x in open(\"MLA_100k_checked_v3.jsonlines\")]\n",
    "    target = lambda x: x.get(\"condition\")\n",
    "    N = -10000\n",
    "    X_train = data[:N]\n",
    "    X_test = data[N:]\n",
    "    y_train = [target(x) for x in X_train]\n",
    "    y_test = [target(x) for x in X_test]\n",
    "    for x in X_test:\n",
    "        del x[\"condition\"]\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading dataset...\")\n",
    "    # Train and test data following sklearn naming conventions\n",
    "    # X_train (X_test too) is a list of dicts with information about each item.\n",
    "    # y_train (y_test too) contains the labels to be predicted (new or used).\n",
    "    # The label of X_train[i] is y_train[i].\n",
    "    # The label of X_test[i] is y_test[i].\n",
    "    X_train, y_train, X_test, y_test = build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "114c986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "######\n",
    "###### Pre-Processing Step\n",
    "#####\n",
    "###########################\n",
    "\n",
    "def step01_preproc(df):\n",
    "    \n",
    "    df = pd.json_normalize(df, sep=\"_\").rename(columns={'id': 'key'})\n",
    "\n",
    "    #### Pivots with aggregations\n",
    "\n",
    "    # non_mercado_pago_payment_methods ######################################\n",
    "    df_normalized = json_normalize(df.to_dict('records'), record_path='non_mercado_pago_payment_methods', meta=[\"key\"])\n",
    "    # Dummies with id  \n",
    "    pivot_df = pd.pivot_table(df_normalized, index='key',\\\n",
    "                              columns=['id'], values='description', aggfunc=len).fillna(0).add_prefix('trat_pay_')\n",
    "    df = pd.merge(df, pivot_df, on='key', how='outer')\n",
    "    # Counting type\n",
    "    pivot_df = pd.pivot_table(df_normalized, index='key',\\\n",
    "                              columns=['type'], values='description', aggfunc=len).fillna(0).fillna(0).add_prefix('trat_pay_')\n",
    "    df = pd.merge(df, pivot_df, on='key', how='outer')\n",
    "\n",
    "    # deal_ids ######################################\n",
    "    df['trat_deal_ids'] = df['deal_ids'].apply(lambda x: bin(0 if x == 'Missing' else 1)[2:])\n",
    "\n",
    "    # variations ######################################\n",
    "    df_normalized = json_normalize(df.to_dict('records'), record_path='variations', meta=[\"key\"])\n",
    "    df_normalized2 = json_normalize(df_normalized.to_dict('records'), record_path='attribute_combinations', meta=[\"key\"])\n",
    "    # seller_custom_field\n",
    "    df_normalized['seller_custom_field_flag'] = df_normalized['seller_custom_field'].apply(lambda x: bin(0 if x == None else 1)[2:])\n",
    "    # num_pictures\n",
    "    df_normalized['num_pictures_ids'] = df_normalized['picture_ids'].apply(lambda x: len(x))\n",
    "    # features to keep first level\n",
    "    pivot_df = pd.pivot_table(df_normalized, index=[\"key\"],\\\n",
    "                              values=[\"available_quantity\", \"sold_quantity\", \"num_pictures_ids\", \"seller_custom_field_flag\"],\\\n",
    "                              aggfunc=\"max\").fillna(0).add_prefix('trat_var_')\n",
    "    df = pd.merge(df, pivot_df, on='key', how='outer')\n",
    "\n",
    "    # features to keep second level\n",
    "    pivot_df = pd.pivot_table(df_normalized2, index='key',\\\n",
    "                              columns=['value_id'], aggfunc=len).fillna(0).add_prefix('trat_var')\n",
    "    pivot_df.columns = [\"_\".join(col).strip() for col in pivot_df.columns.values]\n",
    "    df = pd.merge(df, pivot_df, on='key', how='outer')\n",
    "\n",
    "    # seller historical data ######################################\n",
    "    pivot_df = pd.pivot_table(df, index=[\"seller_id\"],values=[\"key\"],\\\n",
    "                              aggfunc=pd.Series.nunique).fillna(0).add_prefix('trat_sel_')\n",
    "\n",
    "    # attributes ######################################\n",
    "    df_normalized = json_normalize(df.to_dict('records'), record_path='attributes', meta=[\"key\"])\n",
    "    pivot_df = pd.pivot_table(df_normalized, index='key',\\\n",
    "                              columns=[\"attribute_group_id\"],values='attribute_group_name',aggfunc=pd.Series.nunique).fillna(0).add_prefix('trat_attr_')\n",
    "    df = pd.merge(df, pivot_df, on='key', how='outer')\n",
    "\n",
    "    \n",
    "    #### Dummies\n",
    "\n",
    "    # listing_type_id ######################################\n",
    "    dummies = pd.get_dummies(df[\"listing_type_id\"], prefix=\"trat_lis\")\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    # buying_mode ######################################\n",
    "    dummies = pd.get_dummies(df[\"buying_mode\"], prefix=\"trat_buy\")\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    # accepts_mercadopago ######################################\n",
    "    dummies = pd.get_dummies(df[\"accepts_mercadopago\"], prefix=\"trat_mp\")\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    # currency_id ######################################\n",
    "    dummies = pd.get_dummies(df[\"currency_id\"], prefix=\"trat_cur\")\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    # automatic_relist ######################################\n",
    "    dummies = pd.get_dummies(df[\"automatic_relist\"], prefix=\"trat_rel\")\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    # status ######################################\n",
    "    dummies = pd.get_dummies(df[\"status\"], prefix=\"trat_status\")\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    # seller_address_state_id ######################################\n",
    "    dummies = pd.get_dummies(df[\"seller_address_state_id\"], prefix=\"trat_sel_adr\")\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    # shipping_local_pick_up ######################################\n",
    "    dummies = pd.get_dummies(df[\"shipping_local_pick_up\"], prefix=\"trat_ship_pick\")\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    # shipping_free_shipping ######################################\n",
    "    dummies = pd.get_dummies(df[\"shipping_free_shipping\"], prefix=\"trat_ship_free\")\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    # shipping_mode ######################################\n",
    "    dummies = pd.get_dummies(df[\"shipping_mode\"], prefix=\"trat_ship_mode\")\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    # permalink ######################################\n",
    "    regex = r'\\/\\/(.*?)\\.'\n",
    "    df['permalink_aux'] = df['permalink'].apply(lambda x: re.search(regex, x).group(1))\n",
    "    dummies = pd.get_dummies(df[\"permalink_aux\"], prefix=\"trat_perm\")\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "\n",
    "    # last_updated\n",
    "    df['trat_anomes'] = df['last_updated'].apply(lambda x: \"{}{:02d}\".format(*datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\").timetuple()[:2]))\n",
    "    # original_price\n",
    "    df['trat_original_price'] = df['original_price'].fillna(0).apply(lambda x: 1 if x > 0 else 0)\n",
    "    # date_created\n",
    "    df['trat_anomes_date_created'] = df['date_created'].apply(lambda x: \"{}{:02d}\".format(*datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\").timetuple()[:2]))\n",
    "    df['trat_diferenca_data'] = df['trat_anomes'].astype(int) - df['trat_anomes_date_created'].astype(int)\n",
    "    # date_created\n",
    "    df['trat_sell_city_temid'] = df['seller_address_city_id'].apply(lambda x: bin(1 if x == '' else 0)[2:])\n",
    "    df['trat_diferenca_time'] = (((df['stop_time'].astype(int) - df['start_time'].astype(int))/ 1000 / 60 / 60 / 24)).astype(int)\n",
    "    # video_id\n",
    "    df['trat_video_id'] = df['video_id'].apply(lambda x: bin(0 if x == None else 1)[2:])\n",
    "    # tags\n",
    "    df['trat_good_present'] = df['tags'].apply(lambda x: 1 if 'good_quality_thumbnail' in x else 0)\n",
    "\n",
    "    # continuo\n",
    "    # trunc in perc 99 and divide by std after trunc\n",
    "    df['trat_price_c'] = (df['price'].apply(lambda x: 13000 if x > 13000 else x)) / 2725.186005\n",
    "    df['trat_initial_quantity_c'] = (df['initial_quantity'].apply(lambda x: 990 if x > 990 else x)) / 111.134761\n",
    "    df['trat_sold_quantity_c'] = (df['sold_quantity'].apply(lambda x: 41 if x > 41 else x)) / 5.201335\n",
    "    df['trat_available_quantity_c'] = (df['available_quantity'].apply(lambda x: 986 if x > 986 else x)) / 110.436919\n",
    "    \n",
    "\n",
    "    #### Bivariate by category -- trying avoid overfitting\n",
    "\n",
    "    # Worst 90% to 100% used aprox.\n",
    "    codigos = ['MLA85103', 'MLA12205', 'MLA34370', 'MLA15328', 'MLA12204', 'MLA47329',\n",
    "               'MLA31771', 'MLA37759', 'MLA41112', 'MLA2022', 'MLA29389', 'MLA2069',\n",
    "               'MLA10127', 'MLA2038', 'MLA2024', 'MLA44020', 'MLA40547', 'MLA41056',\n",
    "               'MLA3476', 'MLA10937', 'MLA1383', 'MLA41278', 'MLA1207', 'MLA41269',\n",
    "               'MLA3392', 'MLA15210', 'MLA48855']\n",
    "\n",
    "    df['trat_category_id_w1'] = df['category_id'].str.contains('|'.join(codigos)).astype(int)\n",
    "\n",
    "    # Worst 80% to 90% used aprox.\n",
    "    codigos = ['MLA5500', 'MLA15218', 'MLA35923', 'MLA41260', 'MLA5496', 'MLA1963',\n",
    "               'MLA15201', 'MLA41097', 'MLA2032', 'MLA41173', 'MLA41287', 'MLA41064',\n",
    "               'MLA7111', 'MLA3032', 'MLA3991', 'MLA40507', 'MLA41174', 'MLA41083',\n",
    "               'MLA41175', 'MLA41146', 'MLA7373', 'MLA41057', 'MLA40500', 'MLA2458',\n",
    "               'MLA41176', 'MLA1832', 'MLA2070', 'MLA41065', 'MLA9259', 'MLA5492', 'MLA40497']\n",
    "\n",
    "    df['trat_category_id_w2'] = df['category_id'].str.contains('|'.join(codigos)).astype(int)\n",
    "\n",
    "    # Worst 70% to 80% used aprox.\n",
    "    codigos = ['MLA5599', 'MLA15191', 'MLA15157', 'MLA4785', 'MLA7243', 'MLA3031', \n",
    "               'MLA82335', 'MLA31797', 'MLA41192', 'MLA2044', 'MLA11456', 'MLA2040', \n",
    "               'MLA2039', 'MLA41185', 'MLA6087', 'MLA2470', 'MLA15163', 'MLA41149']\n",
    "\n",
    "    df['trat_category_id_w3'] = df['category_id'].str.contains('|'.join(codigos)).astype(int)\n",
    "\n",
    "\n",
    "    # Worst 50% to 60% used aprox.\n",
    "    codigos = ['MLA6009', 'MLA41259', 'MLA1495', 'MLA7251', 'MLA89995', 'MLA370638', \n",
    "               'MLA15171', 'MLA370630', 'MLA40526', 'MLA15172', 'MLA1468', 'MLA1474', \n",
    "               'MLA15197', 'MLA1458', 'MLA15226', 'MLA1227']\n",
    "\n",
    "    df['trat_category_id_w4'] = df['category_id'].str.contains('|'.join(codigos)).astype(int)\n",
    "\n",
    "    # Best 0% to 10% used aprox.\n",
    "    codigos = ['MLA60611', 'MLA352293', 'MLA373016', 'MLA60671', 'MLA352345', 'MLA373122',\n",
    "               'MLA61144', 'MLA45558', 'MLA375228', 'MLA373009', 'MLA60668', 'MLA1429',\n",
    "               'MLA60614', 'MLA374976', 'MLA7859', 'MLA85986', 'MLA60598', 'MLA375067',\n",
    "               'MLA26536', 'MLA3936', 'MLA373299', 'MLA48895', 'MLA73721', 'MLA3931', \n",
    "               'MLA34199', 'MLA117185', 'MLA10076', 'MLA48904', 'MLA85960', 'MLA5375', \n",
    "               'MLA34325', 'MLA22281', 'MLA352333', 'MLA6651', 'MLA86025', 'MLA10216',\n",
    "               'MLA1914', 'MLA372114', 'MLA1590', 'MLA70503', 'MLA119287', 'MLA373335',\n",
    "               'MLA4747', 'MLA86029', 'MLA70504']\n",
    "\n",
    "    df['trat_category_id_b1'] = df['category_id'].str.contains('|'.join(codigos)).astype(int)\n",
    "\n",
    "\n",
    "    # Best 10% to 30% used aprox.\n",
    "    codigos = ['MLA3698', 'MLA6095', 'MLA1642', 'MLA4789', 'MLA12812', 'MLA30216', \n",
    "               'MLA34294', 'MLA1162', 'MLA3529', 'MLA1912', 'MLA1503', 'MLA21086', \n",
    "               'MLA1901', 'MLA1643', 'MLA1607', 'MLA6177', 'MLA2879', 'MLA4734', \n",
    "               'MLA9992', 'MLA3121', 'MLA2980']\n",
    "\n",
    "    df['trat_category_id_b2'] = df['category_id'].str.contains('|'.join(codigos)).astype(int)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df075622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = step01_preproc(X_train)\n",
    "df_test  = step01_preproc(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf88bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "######\n",
    "###### Warranty - WordCount (Embeddings)\n",
    "#####\n",
    "###########################\n",
    "\n",
    "\n",
    "def step02_waranty_transform(df):\n",
    "\n",
    "    # Positives due warranty time specification\n",
    "    palavras = ['1 ano', '2 anos', '5 anos', '3 meses', '4 meses' , '12 meses']\n",
    "    regex = \"|\".join([rf\"\\b{palavra}\\b\" for palavra in palavras])\n",
    "    df[\"trat_warranty_time\"]= df[\"warranty\"].apply(lambda x: '' if x is None else unicodedata.normalize('NFKD', x) \\\n",
    "                                .encode('ASCII', 'ignore').decode('utf-8')) \\\n",
    "                                .str.translate(str.maketrans('', '', string.punctuation)) \\\n",
    "                                .str.contains(regex, regex=True).apply(lambda x: bin(1 if x == True else 0)[2:])\n",
    "\n",
    "    # Positives due Origin\n",
    "    palavras = [\"fabrica\",\"fabricacion\",\"fabricante\",\"directo\",\"directamente\",\"novo\",\"oficial\",\"nuevo\"]\n",
    "    regex = \"|\".join([rf\"\\b{palavra}\\b\" for palavra in palavras])\n",
    "    df[\"trat_warranty_fabric\"]= df[\"warranty\"].apply(lambda x: '' if x is None else unicodedata.normalize('NFKD', x) \\\n",
    "                                .encode('ASCII', 'ignore').decode('utf-8')) \\\n",
    "                                .str.translate(str.maketrans('', '', string.punctuation)) \\\n",
    "                                .str.contains(regex, regex=True).apply(lambda x: bin(1 if x == True else 0)[2:])\n",
    "    \n",
    "    # Negative due reputation needs check\n",
    "    palavras = [\"reputacion\" ,\"calificacion\",\"calificaciones\",\"sin garantia\",\"revisan\", \"revisados\", \"buen\", \"buenas\", \"corresponden\",\"perfecto\",\"articulo\",\"descripcion\",\"revisa\",\"completo\"]\n",
    "    regex = \"|\".join([rf\"\\b{palavra}\\b\" for palavra in palavras])\n",
    "    df[\"trat_warranty_reputation\"]= df[\"warranty\"].apply(lambda x: '' if x is None else unicodedata.normalize('NFKD', x) \\\n",
    "                                .encode('ASCII', 'ignore').decode('utf-8')) \\\n",
    "                                .str.translate(str.maketrans('', '', string.punctuation)) \\\n",
    "                                .str.contains(regex, regex=True).apply(lambda x: bin(1 if x == True else 0)[2:])\n",
    "    \n",
    "    # Negative due image needs\n",
    "    palavras = [\"foto\",\"fotos\",\"imagem\",\"imagenes\"]\n",
    "    regex = \"|\".join([rf\"\\b{palavra}\\b\" for palavra in palavras])\n",
    "    df[\"trat_warranty_image\"]= df[\"warranty\"].apply(lambda x: '' if x is None else unicodedata.normalize('NFKD', x) \\\n",
    "                                .encode('ASCII', 'ignore').decode('utf-8')) \\\n",
    "                                .str.translate(str.maketrans('', '', string.punctuation)) \\\n",
    "                                .str.contains(regex, regex=True).apply(lambda x: bin(1 if x == True else 0)[2:])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a65d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = step02_waranty_transform(df_train)\n",
    "df_test  = step02_waranty_transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f8ddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "######\n",
    "###### Replib for title field - train\n",
    "#####\n",
    "###########################\n",
    "\n",
    "def step03_replib_train(df_train):\n",
    "    \n",
    "    \n",
    "    ## Training Step       \n",
    "    # RF\n",
    "    params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 10,\n",
    "        'min_samples_leaf': 5,\n",
    "        'max_features': 'sqrt',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = make_pipeline(\n",
    "        MetaProd2Vec(pre_fitted_version=\"MLA_feb21_dim100\")\n",
    "        ,RandomForestClassifier(**params)\n",
    "    )\n",
    "    \n",
    "    targets = df_train['condition']\n",
    "    titles = df_train[\"title\"].fillna('XXXX').tolist()\n",
    "    items = [{\"title\": title} for title in titles]\n",
    "    \n",
    "    return model.fit(items, targets)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f51f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_replib = step03_replib_train(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1880f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "######\n",
    "###### Replib for title field - predict\n",
    "#####\n",
    "###########################\n",
    "\n",
    "def step04_replib_predict(df_train,df_test,model_replib):\n",
    "    \n",
    "    titles_train = df_train[\"title\"].fillna('XXXX').tolist()\n",
    "    items_train = [{\"title\": title} for title in titles_train]\n",
    "    df_train['trat_rep_title'] = model_replib.predict_proba(items_train)[:, 0]\n",
    "    \n",
    "    titles_test = df_test[\"title\"].fillna('XXXX').tolist()\n",
    "    items_test = [{\"title\": title} for title in titles_test]\n",
    "    df_test['trat_rep_title'] = model_replib.predict_proba(items_test)[:, 0]\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45598a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = step04_replib_predict(df_train,df_test,model_replib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a296430",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "######\n",
    "###### Clean unused columns and equalize dataset according to traiset\n",
    "#####\n",
    "###########################\n",
    "\n",
    "def step05_clean_dataframes(df_train,df_test):\n",
    "    \n",
    "    df_train.set_index('key', inplace=True)\n",
    "    df_test.set_index('key', inplace=True)\n",
    "    \n",
    "    df_train = df_train.filter(regex='^(condition|trat_)').fillna(-1)\n",
    "    df_test = df_test.filter(regex='^(condition|trat_)').fillna(-1)\n",
    "    \n",
    "    # Testset with same columns trainset\n",
    "    \n",
    "    new_cols = set(df_train.drop(columns=['condition']).columns) - set(df_test.columns)\n",
    "\n",
    "    # adicionar colunas faltantes em B e preencher com -1\n",
    "    for col in new_cols:\n",
    "        df_test[col] = -1\n",
    "    \n",
    "    # remover colunas de B que não estão presentes em A\n",
    "    for col in df_test.columns:\n",
    "        if col not in df_train.drop(columns=['condition']).columns:\n",
    "            df_test.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    # reordenar colunas de B para ficar igual a A\n",
    "        df_test = df_test[df_train.drop(columns=['condition']).columns].copy()\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcfb31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = step05_clean_dataframes(df_train,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4fe7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "######\n",
    "###### Train final model\n",
    "#####\n",
    "###########################\n",
    "\n",
    "def step06_model_train(df_train):\n",
    "    \n",
    "    \n",
    "    ## Training Step       \n",
    "    # RF\n",
    "    params = {\n",
    "        'bootstrap': True, \n",
    "        'ccp_alpha': 0.0, \n",
    "        'class_weight': None,\n",
    "        'criterion': 'gini', \n",
    "        'max_depth': None, \n",
    "        'max_features': 'sqrt',\n",
    "        'max_leaf_nodes': None, \n",
    "        'max_samples': None,\n",
    "        'min_impurity_decrease': 0.0, \n",
    "        'min_samples_leaf': 1,\n",
    "        'min_samples_split': 2, \n",
    "        'min_weight_fraction_leaf': 0.0,\n",
    "        'n_estimators': 100, \n",
    "        'n_jobs': -1, \n",
    "        'oob_score': False,\n",
    "        'random_state': 123, \n",
    "        'verbose': 0, \n",
    "        'warm_start': False\n",
    "\n",
    "    }\n",
    "    \n",
    "    model = make_pipeline(\n",
    "        RandomForestClassifier(**params)\n",
    "    )\n",
    "    \n",
    "    targets = df_train['condition']\n",
    "    X = df_train.drop(columns=['condition'])\n",
    "\n",
    "    return model.fit(X, targets)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7df6e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = step06_model_train(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0cfc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_final.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1820d6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score =  0.8822\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score = \", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff455e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
